{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EN3160 Assignment 3 on Neural Networks\n",
    "\n",
    "Instructed by Dr. Ranga Rodrigo\n",
    "\n",
    "Done by Jayakumar W.S. (210236P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This assignment is focused on implementing neural networks for image classification. This is done by using:\n",
    "1. Our own neural network implementation\n",
    "2. An implementation of LeNet-5\n",
    "3. An implementation of ResNet-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose ([ transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5) , (0.5, 0.5, 0.5))])\n",
    "batch_size = 32\n",
    "trainset = torchvision.datasets.CIFAR10(root= './data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root= './data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device begin used : {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our own architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Din = 3*32*32 # Input size (flattened CIFAR=10 image size)\n",
    "K = 10 # Output size (number of classes in CIFAR=10)\n",
    "std = 1e-5\n",
    "# Initialize weights and biases\n",
    "w = torch.randn(Din, K, device=device, dtype=torch.float, requires_grad=True) * std\n",
    "b = torch.randn(K, device=device, dtype=torch.float, requires_grad=True)\n",
    "# Hyperparameters\n",
    "iterations = 20\n",
    "lr = 2e-6 # Learning rate\n",
    "lr_decay = 0.9 # Learning rate decay\n",
    "reg = 0 # Regularization\n",
    "loss_history = [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(iterations):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = data\n",
    "        Ntr = inputs.shape[0]  # Batch size\n",
    "        x_train = inputs.view(Ntr, -1).to(device)  # Flatten input to (Ntr, Din)\n",
    "        y_train_onehot = nn.functional.one_hot(labels, K).float().to(device)  # Convert labels to one-hot\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = x_train.mm(w) + b  # Output layer activation\n",
    "\n",
    "        # Loss calculation (Mean Squared Error with regularization)\n",
    "        loss = (1/Ntr) * torch.sum((y_pred - y_train_onehot) ** 2) + reg * torch.sum(w ** 2)\n",
    "        loss_history.append(loss.item())\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        dy_pred = (2.0 / Ntr) * (y_pred - y_train_onehot)\n",
    "        dw = x_train.t().mm(dy_pred) + reg * w\n",
    "        db = dy_pred.sum(dim=0)\n",
    "\n",
    "        # Parameter update\n",
    "        w = w - lr * dw\n",
    "        b = b - lr * db\n",
    "\n",
    "    print(f\"Epoch {t + 1} / {iterations}, Loss: {running_loss / len(trainloader)}\")\n",
    "\n",
    "    # Learning rate decay\n",
    "    lr *= lr_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del w, b, x_train, y_train_onehot, y_pred, loss, dy_pred, dw, db\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This implementation is not efficient and is only for educational purposes. For real-world applications, use PyTorch's built-in functions and classes. This fails\n",
    "# as memory usage increases with the number of iterations.\n",
    "\n",
    "Din = 3*32*32 # Input size (flattened CIFAR=10 image size)\n",
    "K = 10 # Output size (number of classes in CIFAR=10)\n",
    "std = 1e-5\n",
    "# Initialize weights and biases\n",
    "w1 = torch.randn(Din, 100, device=device, requires_grad=True)\n",
    "b1 = torch.zeros(100, device=device, requires_grad=True)\n",
    "w2 = torch.randn(100, K, device=device, requires_grad=True)\n",
    "b2 = torch.zeros(K, device=device, requires_grad=True)\n",
    "# Hyperparameters\n",
    "iterations = 20\n",
    "lr = 2e-6 # Learning rate\n",
    "lr_decay = 0.9 # Learning rate decay\n",
    "reg = 0 # Regularization\n",
    "loss_history = [ ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(iterations) :\n",
    "    running_loss = 0.0\n",
    "    for i , data in enumerate(trainloader, 0) :\n",
    "        # Get inputs and labe l s\n",
    "        inputs , labels = data\n",
    "        Ntr = inputs.shape[0] # Batch size\n",
    "        x_train = inputs.view(Ntr, -1).to(device) # Flatten input to (Ntr, Din)\n",
    "        y_train_onehot = nn.functional.one_hot(labels, K).float().to(device) # Convert labe l s to one=hot # Forward pass\n",
    "        hidden = x_train.mm(w1) + b1\n",
    "        y_pred = hidden.mm(w2) + b2\n",
    "        # Loss calculation (Mean Squared Error with regularization)\n",
    "        loss = (1/Ntr) * torch.sum((y_pred - y_train_onehot) ** 2) + reg * (torch.sum(w1 ** 2) + torch.sum(w2 ** 2))\n",
    "        loss_history.append(loss.item())\n",
    "        running_loss += loss.item()\n",
    "        # Backpropagation\n",
    "        dy_pred = (2.0 / Ntr) * (y_pred - y_train_onehot)\n",
    "        dhidden = dy_pred.mm(w2.t()) \n",
    "        dw2 = hidden.t().mm(dy_pred) + reg * w2\n",
    "        db2 = dy_pred.sum(dim=0)\n",
    "        dw1 = x_train.t().mm(dhidden) + reg * w1\n",
    "        db1 = dhidden.sum(dim=0)\n",
    "        # Parameter update\n",
    "        w2 = w2 - lr * dw2\n",
    "        b2 = b2 - lr * db2\n",
    "        w1 = w1 - lr * dw1\n",
    "        b1 = b1 - lr * db1\n",
    "    print(f\"Epoch {t+1} / {iterations} , Loss : {running_loss/len(trainloader)}\")\n",
    "    # Learning rat e decay\n",
    "    lr *= lr_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del w1, b1, w2, b2, x_train, y_train_onehot, y_pred, loss, dy_pred, dhidden, dw2, db2, dw1, db1\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, Din, H, Dout):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(Din, H)\n",
    "        self.linear2 = nn.Linear(H, Dout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(Din, 100, K).to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(iterations):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = data\n",
    "        Ntr = inputs.shape[0]  # Batch size\n",
    "        x_train = inputs.view(Ntr, -1).to(device)  # Flatten input to (Ntr, Din)\n",
    "        y_train = labels.to(device)  # Convert labels to one-hot\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x_train)\n",
    "\n",
    "        # Loss calculation\n",
    "        loss_val = loss(y_pred, y_train)\n",
    "        loss_history.append(loss_val.item())\n",
    "        running_loss += loss_val.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {t + 1} / {iterations}, Loss: {running_loss / len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "model.eval()\n",
    "with  torch.inference_mode():\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data\n",
    "        x_test, y_test = inputs.to(device), labels.to(device)\n",
    "        y_pred = model(x_test)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        accuracy += (predicted == y_test).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {accuracy / len(testset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "classes = tuple(str(i) for i in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, input_size, input_channels, output_size):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 6, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        conv_output_size = ((input_size - 4) // 2 - 4) // 2\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16 * conv_output_size * conv_output_size, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.conv2(y)\n",
    "        y = y.view(y.size(0), -1)\n",
    "        y = self.classifier(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet(input_size = 28, input_channels = 1, output_size = 10).to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_history = [ ]\n",
    "iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10, Loss: 0.22979344264939428\n",
      "Epoch 2 / 10, Loss: 0.06832440779755512\n",
      "Epoch 3 / 10, Loss: 0.04808383764217918\n",
      "Epoch 4 / 10, Loss: 0.03904317057208003\n",
      "Epoch 5 / 10, Loss: 0.03111796266750122\n",
      "Epoch 6 / 10, Loss: 0.027499199597144617\n",
      "Epoch 7 / 10, Loss: 0.02319336009456941\n",
      "Epoch 8 / 10, Loss: 0.01949250753870971\n",
      "Epoch 9 / 10, Loss: 0.01712961347642146\n",
      "Epoch 10 / 10, Loss: 0.014968328931884146\n"
     ]
    }
   ],
   "source": [
    "for t in range(iterations):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = data\n",
    "        x_train, y_train = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x_train)\n",
    "\n",
    "        # Loss calculation\n",
    "        loss_val = loss(y_pred, y_train)\n",
    "        loss_history.append(loss_val.item())\n",
    "        running_loss += loss_val.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {t + 1} / {iterations}, Loss: {running_loss / len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9897\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "model.eval()\n",
    "with  torch.inference_mode():\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data\n",
    "        x_test, y_test = inputs.to(device), labels.to(device)\n",
    "        y_pred = model(x_test)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        accuracy += (predicted == y_test).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {accuracy / len(testset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, loss, optimizer, x_train, y_train, y_pred, loss_val\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing ResNet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ants', 'bees']\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet18(weights = 'IMAGENET1K_V1').to(device)\n",
    "data_folder = './data/hymenoptera_data'\n",
    "train_transforms = transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "trainset = torchvision.datasets.ImageFolder(root=f'{data_folder}/train', transform=train_transforms)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "test_transforms = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "testset = torchvision.datasets.ImageFolder(root=f'{data_folder}/val', transform=test_transforms)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "classes = trainset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_history = [ ]\n",
    "iterations = 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
