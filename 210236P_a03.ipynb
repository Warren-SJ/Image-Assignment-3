{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EN3160 Assignment 3 on Neural Networks\n",
    "\n",
    "Instructed by Dr. Ranga Rodrigo\n",
    "\n",
    "Done by Jayakumar W.S. (210236P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This assignment is focused on implementing neural networks for image classification. This is done by using:\n",
    "1. Our own neural network implementation\n",
    "2. An implementation of LeNet-5\n",
    "3. An implementation of ResNet-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose ([ transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5) , (0.5, 0.5, 0.5))])\n",
    "batch_size = 50\n",
    "trainset = torchvision.datasets.CIFAR10(root= './ data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root= './ data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device begin used : {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our own architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Din = 3*32*32 # Input size (flattened CIFAR=10 image size)\n",
    "K = 10 # Output size (number of classes in CIFAR=10)\n",
    "std = 1e-5\n",
    "# Initialize weights and biases\n",
    "w = torch.randn(Din, K, device=device, dtype=torch.float, requires_grad=True) * std\n",
    "b = torch.randn(K, device=device, dtype=torch.float, requires_grad=True)\n",
    "# Hyperparameters\n",
    "iterations = 20\n",
    "lr = 2e-6 # Learning rate\n",
    "lr_decay = 0.9 # Learning rate decay\n",
    "reg = 0 # Regularization\n",
    "loss_history = [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(iterations):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = data\n",
    "        Ntr = inputs.shape[0]  # Batch size\n",
    "        x_train = inputs.view(Ntr, -1).to(device)  # Flatten input to (Ntr, Din)\n",
    "        y_train_onehot = nn.functional.one_hot(labels, K).float().to(device)  # Convert labels to one-hot\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = x_train.mm(w) + b  # Output layer activation\n",
    "\n",
    "        # Loss calculation (Mean Squared Error with regularization)\n",
    "        loss = (1/Ntr) * torch.sum((y_pred - y_train_onehot) ** 2) + reg * torch.sum(w ** 2)\n",
    "        loss_history.append(loss.item())\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        dy_pred = (2.0 / Ntr) * (y_pred - y_train_onehot)\n",
    "        dw = x_train.t().mm(dy_pred) + reg * w\n",
    "        db = dy_pred.sum(dim=0)\n",
    "\n",
    "        # Parameter update\n",
    "        w = w - lr * dw\n",
    "        b = b - lr * db\n",
    "\n",
    "    print(f\"Epoch {t + 1} / {iterations}, Loss: {running_loss / len(trainloader)}\")\n",
    "\n",
    "    # Learning rate decay\n",
    "    lr *= lr_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Din = 3*32*32 # Input size (flattened CIFAR=10 image size)\n",
    "K = 10 # Output size (number of classes in CIFAR=10)\n",
    "std = 1e-5\n",
    "# Initialize weights and biases\n",
    "w1 = torch.randn(Din, 100, device=device, requires_grad=True)\n",
    "b1 = torch.zeros(100, device=device, requires_grad=True)\n",
    "w2 = torch.randn(100, K, device=device, requires_grad=True)\n",
    "b2 = torch.zeros(K, device=device, requires_grad=True)\n",
    "# Hyperparameters\n",
    "iterations = 20\n",
    "lr = 2e-6 # Learning rate\n",
    "lr_decay = 0.9 # Learning rate decay\n",
    "reg = 0 # Regularization\n",
    "loss_history = [ ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(iterations) :\n",
    "    running_loss = 0.0\n",
    "    for i , data in enumerate(trainloader, 0) :\n",
    "        # Get inputs and labe l s\n",
    "        inputs , labels = data\n",
    "        Ntr = inputs.shape[0] # Batch size\n",
    "        x_train = inputs.view(Ntr, -1).to(device) # Flatten input to (Ntr, Din)\n",
    "        y_train_onehot = nn.functional.one_hot(labels, K).float().to(device) # Convert labe l s to one=hot # Forward pass\n",
    "        hidden = x_train.mm(w1) + b1\n",
    "        y_pred = hidden.mm(w2) + b2\n",
    "        # Loss calculation (Mean Squared Error with regularization)\n",
    "        loss = (1/Ntr)*torch.sum((y_pred - y_train_onehot) ** 2)+reg*torch.sum(w**2) \n",
    "        loss_history.append(loss.item())\n",
    "        running_loss += loss.item()\n",
    "        # Backpropagation\n",
    "        dy_pred = (2.0 / Ntr) * (y_pred - y_train_onehot)\n",
    "        dhidden = dy_pred.mm(w2.t()) \n",
    "        dw2 = hidden.t().mm(dy_pred) + reg * w2\n",
    "        db2 = dy_pred.sum(dim=0)\n",
    "        dw1 = x_train.t().mm(dhidden) + reg * w1\n",
    "        db1 = dhidden.sum(dim=0)\n",
    "        # Parameter update\n",
    "        w2 = w2 - lr * dw2\n",
    "        b2 = b2 - lr * db2\n",
    "        w1 = w1 - lr * dw1\n",
    "        b1 = b1 - lr * db1\n",
    "    print(f\"Epoch {t+1} / {iterations} , Loss : {running_loss/len(trainloader)}\")\n",
    "    # Learning rat e decay\n",
    "    lr *= lr_decay"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
