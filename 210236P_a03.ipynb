{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EN3160 Assignment 3 on Neural Networks\n",
    "\n",
    "Instructed by Dr. Ranga Rodrigo\n",
    "\n",
    "Done by Jayakumar W.S. (210236P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This assignment is focused on implementing neural networks for image classification. This is done by using:\n",
    "1. Our own neural network implementation\n",
    "2. An implementation of LeNet-5\n",
    "3. An implementation of ResNet-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [13:45<00:00, 206435.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Device begin used : cuda\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose ([ transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5) , (0.5, 0.5, 0.5))])\n",
    "batch_size = 32\n",
    "trainset = torchvision.datasets.CIFAR10(root= './data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root= './data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device begin used : {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our own architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Din = 3*32*32 # Input size (flattened CIFAR=10 image size)\n",
    "K = 10 # Output size (number of classes in CIFAR=10)\n",
    "std = 1e-5\n",
    "# Initialize weights and biases\n",
    "w = torch.randn(Din, K, device=device, dtype=torch.float, requires_grad=True) * std\n",
    "b = torch.randn(K, device=device, dtype=torch.float, requires_grad=True)\n",
    "# Hyperparameters\n",
    "iterations = 20\n",
    "lr = 2e-6 # Learning rate\n",
    "lr_decay = 0.9 # Learning rate decay\n",
    "reg = 0 # Regularization\n",
    "loss_history = [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 20, Loss: 12.58823975827247\n",
      "Epoch 2 / 20, Loss: 11.040970125033622\n",
      "Epoch 3 / 20, Loss: 10.36810743206217\n",
      "Epoch 4 / 20, Loss: 9.981834405824609\n",
      "Epoch 5 / 20, Loss: 9.722565463621953\n",
      "Epoch 6 / 20, Loss: 9.529328597331764\n",
      "Epoch 7 / 20, Loss: 9.375695554095053\n",
      "Epoch 8 / 20, Loss: 9.253262897828261\n",
      "Epoch 9 / 20, Loss: 9.14980258319291\n",
      "Epoch 10 / 20, Loss: 9.063382420567313\n",
      "Epoch 11 / 20, Loss: 8.986990968004985\n",
      "Epoch 12 / 20, Loss: 8.922773878618608\n",
      "Epoch 13 / 20, Loss: 8.866755202727218\n",
      "Epoch 14 / 20, Loss: 8.818267561118724\n",
      "Epoch 15 / 20, Loss: 8.776347569754241\n",
      "Epoch 16 / 20, Loss: 8.737724349960942\n",
      "Epoch 17 / 20, Loss: 8.705339573624038\n",
      "Epoch 18 / 20, Loss: 8.676764998005813\n",
      "Epoch 19 / 20, Loss: 8.650618852748371\n",
      "Epoch 20 / 20, Loss: 8.627122077282925\n"
     ]
    }
   ],
   "source": [
    "for t in range(iterations):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = data\n",
    "        Ntr = inputs.shape[0]  # Batch size\n",
    "        x_train = inputs.view(Ntr, -1).to(device)  # Flatten input to (Ntr, Din)\n",
    "        y_train_onehot = nn.functional.one_hot(labels, K).float().to(device)  # Convert labels to one-hot\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = x_train.mm(w) + b  # Output layer activation\n",
    "\n",
    "        # Loss calculation (Mean Squared Error with regularization)\n",
    "        loss = (1/Ntr) * torch.sum((y_pred - y_train_onehot) ** 2) + reg * torch.sum(w ** 2)\n",
    "        loss_history.append(loss.item())\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        dy_pred = (2.0 / Ntr) * (y_pred - y_train_onehot)\n",
    "        dw = x_train.t().mm(dy_pred) + reg * w\n",
    "        db = dy_pred.sum(dim=0)\n",
    "\n",
    "        # Parameter update\n",
    "        w = w - lr * dw\n",
    "        b = b - lr * db\n",
    "\n",
    "    print(f\"Epoch {t + 1} / {iterations}, Loss: {running_loss / len(trainloader)}\")\n",
    "\n",
    "    # Learning rate decay\n",
    "    lr *= lr_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Din = 3*32*32 # Input size (flattened CIFAR=10 image size)\n",
    "K = 10 # Output size (number of classes in CIFAR=10)\n",
    "std = 1e-5\n",
    "# Initialize weights and biases\n",
    "w1 = torch.randn(Din, 100, device=device, requires_grad=True)\n",
    "b1 = torch.zeros(100, device=device, requires_grad=True)\n",
    "w2 = torch.randn(100, K, device=device, requires_grad=True)\n",
    "b2 = torch.zeros(K, device=device, requires_grad=True)\n",
    "# Hyperparameters\n",
    "iterations = 20\n",
    "lr = 2e-6 # Learning rate\n",
    "lr_decay = 0.9 # Learning rate decay\n",
    "reg = 0 # Regularization\n",
    "loss_history = [ ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 20 , Loss : 62761.097278245965\n",
      "Epoch 2 / 20 , Loss : 12057.843131760337\n",
      "Epoch 3 / 20 , Loss : 5480.4051508204875\n",
      "Epoch 4 / 20 , Loss : 3028.8498120445206\n",
      "Epoch 5 / 20 , Loss : 1866.6540542182752\n",
      "Epoch 6 / 20 , Loss : 1239.1462293394238\n",
      "Epoch 7 / 20 , Loss : 870.2966193981988\n",
      "Epoch 8 / 20 , Loss : 639.2929228271953\n",
      "Epoch 9 / 20 , Loss : 487.3385212044646\n",
      "Epoch 10 / 20 , Loss : 383.4728423720816\n",
      "Epoch 11 / 20 , Loss : 309.9592100644981\n",
      "Epoch 12 / 20 , Loss : 256.5315048598511\n",
      "Epoch 13 / 20 , Loss : 216.70926232316597\n",
      "Epoch 14 / 20 , Loss : 186.42874668777867\n",
      "Epoch 15 / 20 , Loss : 162.99634342596306\n",
      "Epoch 16 / 20 , Loss : 144.50509007039622\n",
      "Epoch 17 / 20 , Loss : 129.7724288188717\n",
      "Epoch 18 / 20 , Loss : 117.8400370475198\n",
      "Epoch 19 / 20 , Loss : 108.07219468578649\n",
      "Epoch 20 / 20 , Loss : 100.00710443068374\n"
     ]
    }
   ],
   "source": [
    "for t in range(iterations) :\n",
    "    running_loss = 0.0\n",
    "    for i , data in enumerate(trainloader, 0) :\n",
    "        # Get inputs and labe l s\n",
    "        inputs , labels = data\n",
    "        Ntr = inputs.shape[0] # Batch size\n",
    "        x_train = inputs.view(Ntr, -1).to(device) # Flatten input to (Ntr, Din)\n",
    "        y_train_onehot = nn.functional.one_hot(labels, K).float().to(device) # Convert labe l s to one=hot # Forward pass\n",
    "        hidden = x_train.mm(w1) + b1\n",
    "        y_pred = hidden.mm(w2) + b2\n",
    "        # Loss calculation (Mean Squared Error with regularization)\n",
    "        loss = (1/Ntr) * torch.sum((y_pred - y_train_onehot) ** 2) + reg * (torch.sum(w1 ** 2) + torch.sum(w2 ** 2))\n",
    "        loss_history.append(loss.item())\n",
    "        running_loss += loss.item()\n",
    "        # Backpropagation\n",
    "        dy_pred = (2.0 / Ntr) * (y_pred - y_train_onehot)\n",
    "        dhidden = dy_pred.mm(w2.t()) \n",
    "        dw2 = hidden.t().mm(dy_pred) + reg * w2\n",
    "        db2 = dy_pred.sum(dim=0)\n",
    "        dw1 = x_train.t().mm(dhidden) + reg * w1\n",
    "        db1 = dhidden.sum(dim=0)\n",
    "        # Parameter update\n",
    "        w2 = w2 - lr * dw2\n",
    "        b2 = b2 - lr * db2\n",
    "        w1 = w1 - lr * dw1\n",
    "        b1 = b1 - lr * db1\n",
    "    print(f\"Epoch {t+1} / {iterations} , Loss : {running_loss/len(trainloader)}\")\n",
    "    # Learning rat e decay\n",
    "    lr *= lr_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, Din, H, Dout):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(Din, H)\n",
    "        self.linear2 = nn.Linear(H, Dout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(Din, 100, K).to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 20, Loss: 2.1908527879629545\n",
      "Epoch 2 / 20, Loss: 2.059300367220502\n",
      "Epoch 3 / 20, Loss: 1.9850221211835504\n",
      "Epoch 4 / 20, Loss: 1.9329886095537563\n",
      "Epoch 5 / 20, Loss: 1.8936424514687527\n",
      "Epoch 6 / 20, Loss: 1.8625754042458855\n",
      "Epoch 7 / 20, Loss: 1.8372612156626968\n",
      "Epoch 8 / 20, Loss: 1.8161037901007664\n",
      "Epoch 9 / 20, Loss: 1.797995209312561\n",
      "Epoch 10 / 20, Loss: 1.782369407490897\n",
      "Epoch 11 / 20, Loss: 1.7683418646731288\n",
      "Epoch 12 / 20, Loss: 1.7560087997640315\n",
      "Epoch 13 / 20, Loss: 1.7448427204283399\n",
      "Epoch 14 / 20, Loss: 1.7345941515969527\n",
      "Epoch 15 / 20, Loss: 1.7251037038142911\n",
      "Epoch 16 / 20, Loss: 1.716438345625396\n",
      "Epoch 17 / 20, Loss: 1.7080758406577474\n",
      "Epoch 18 / 20, Loss: 1.7002481696549243\n",
      "Epoch 19 / 20, Loss: 1.6929249335616656\n",
      "Epoch 20 / 20, Loss: 1.6859852562939137\n"
     ]
    }
   ],
   "source": [
    "for t in range(iterations):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = data\n",
    "        Ntr = inputs.shape[0]  # Batch size\n",
    "        x_train = inputs.view(Ntr, -1).to(device)  # Flatten input to (Ntr, Din)\n",
    "        y_train = labels.to(device)  # Convert labels to one-hot\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x_train)\n",
    "\n",
    "        # Loss calculation\n",
    "        loss_val = loss(y_pred, y_train)\n",
    "        loss_history.append(loss_val.item())\n",
    "        running_loss += loss_val.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {t + 1} / {iterations}, Loss: {running_loss / len(trainloader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
