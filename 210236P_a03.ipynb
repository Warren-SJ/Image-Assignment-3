{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EN3160 Assignment 3 on Neural Networks\n",
    "\n",
    "Instructed by Dr. Ranga Rodrigo\n",
    "\n",
    "Done by Jayakumar W.S. (210236P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This assignment is focused on implementing neural networks for image classification. This is done by using:\n",
    "1. Our own neural network implementation\n",
    "2. An implementation of LeNet-5\n",
    "3. An implementation of ResNet-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Device begin used : cuda\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose ([ transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5) , (0.5, 0.5, 0.5))])\n",
    "batch_size = 32\n",
    "trainset = torchvision.datasets.CIFAR10(root= './data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root= './data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device begin used : {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our own architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Din = 3*32*32 # Input size (flattened CIFAR=10 image size)\n",
    "K = 10 # Output size (number of classes in CIFAR=10)\n",
    "std = 1e-5\n",
    "# Initialize weights and biases\n",
    "w = torch.randn(Din, K, device=device, dtype=torch.float, requires_grad=True) * std\n",
    "b = torch.randn(K, device=device, dtype=torch.float, requires_grad=True)\n",
    "# Hyperparameters\n",
    "iterations = 20\n",
    "lr = 2e-6 # Learning rate\n",
    "lr_decay = 0.9 # Learning rate decay\n",
    "reg = 0 # Regularization\n",
    "loss_history = [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 20, Loss: 15.62130003713753\n",
      "Epoch 2 / 20, Loss: 13.63142822800122\n",
      "Epoch 3 / 20, Loss: 12.764943606717726\n",
      "Epoch 4 / 20, Loss: 12.269712213209937\n",
      "Epoch 5 / 20, Loss: 11.938187017398086\n",
      "Epoch 6 / 20, Loss: 11.688047737321714\n",
      "Epoch 7 / 20, Loss: 11.494783885038135\n",
      "Epoch 8 / 20, Loss: 11.33645681257974\n",
      "Epoch 9 / 20, Loss: 11.20408751121028\n",
      "Epoch 10 / 20, Loss: 11.09526105637895\n",
      "Epoch 11 / 20, Loss: 11.000283862716177\n",
      "Epoch 12 / 20, Loss: 10.91800398408642\n",
      "Epoch 13 / 20, Loss: 10.848268118411093\n",
      "Epoch 14 / 20, Loss: 10.787987606539149\n",
      "Epoch 15 / 20, Loss: 10.734826364657549\n",
      "Epoch 16 / 20, Loss: 10.687445311384634\n",
      "Epoch 17 / 20, Loss: 10.644645016390164\n",
      "Epoch 18 / 20, Loss: 10.6083836052102\n",
      "Epoch 19 / 20, Loss: 10.574704309037612\n",
      "Epoch 20 / 20, Loss: 10.545810999659796\n"
     ]
    }
   ],
   "source": [
    "for t in range(iterations):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = data\n",
    "        Ntr = inputs.shape[0]  # Batch size\n",
    "        x_train = inputs.view(Ntr, -1).to(device)  # Flatten input to (Ntr, Din)\n",
    "        y_train_onehot = nn.functional.one_hot(labels, K).float().to(device)  # Convert labels to one-hot\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = x_train.mm(w) + b  # Output layer activation\n",
    "\n",
    "        # Loss calculation (Mean Squared Error with regularization)\n",
    "        loss = (1/Ntr) * torch.sum((y_pred - y_train_onehot) ** 2) + reg * torch.sum(w ** 2)\n",
    "        loss_history.append(loss.item())\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        dy_pred = (2.0 / Ntr) * (y_pred - y_train_onehot)\n",
    "        dw = x_train.t().mm(dy_pred) + reg * w\n",
    "        db = dy_pred.sum(dim=0)\n",
    "\n",
    "        # Parameter update\n",
    "        w = w - lr * dw\n",
    "        b = b - lr * db\n",
    "\n",
    "    print(f\"Epoch {t + 1} / {iterations}, Loss: {running_loss / len(trainloader)}\")\n",
    "\n",
    "    # Learning rate decay\n",
    "    lr *= lr_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del w, b, x_train, y_train_onehot, y_pred, loss, dy_pred, dw, db\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This implementation is not efficient and is only for educational purposes. For real-world applications, use PyTorch's built-in functions and classes. This fails\n",
    "# as memory usage increases with the number of iterations.\n",
    "\n",
    "Din = 3*32*32 # Input size (flattened CIFAR=10 image size)\n",
    "K = 10 # Output size (number of classes in CIFAR=10)\n",
    "std = 1e-5\n",
    "# Initialize weights and biases\n",
    "w1 = torch.randn(Din, 100, device=device, requires_grad=True)\n",
    "b1 = torch.zeros(100, device=device, requires_grad=True)\n",
    "w2 = torch.randn(100, K, device=device, requires_grad=True)\n",
    "b2 = torch.zeros(K, device=device, requires_grad=True)\n",
    "# Hyperparameters\n",
    "iterations = 20\n",
    "lr = 2e-6 # Learning rate\n",
    "lr_decay = 0.9 # Learning rate decay\n",
    "reg = 0 # Regularization\n",
    "loss_history = [ ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 20 , Loss : 58862.28696067259\n",
      "Epoch 2 / 20 , Loss : 11133.438430952096\n",
      "Epoch 3 / 20 , Loss : 5069.310151688959\n",
      "Epoch 4 / 20 , Loss : 2819.410859774872\n",
      "Epoch 5 / 20 , Loss : 1754.4436296010276\n",
      "Epoch 6 / 20 , Loss : 1178.6637748186206\n",
      "Epoch 7 / 20 , Loss : 838.9237689642439\n",
      "Epoch 8 / 20 , Loss : 624.9427198335595\n",
      "Epoch 9 / 20 , Loss : 483.2508021066071\n",
      "Epoch 10 / 20 , Loss : 385.50470730225703\n",
      "Epoch 11 / 20 , Loss : 315.80564707574825\n",
      "Epoch 12 / 20 , Loss : 264.682856671412\n",
      "Epoch 13 / 20 , Loss : 226.30642076371498\n",
      "Epoch 14 / 20 , Loss : 196.89464548102418\n",
      "Epoch 15 / 20 , Loss : 173.9280351478933\n",
      "Epoch 16 / 20 , Loss : 155.69256878814406\n",
      "Epoch 17 / 20 , Loss : 141.04625540334905\n",
      "Epoch 18 / 20 , Loss : 129.10847062631365\n",
      "Epoch 19 / 20 , Loss : 119.32102659766062\n",
      "Epoch 20 / 20 , Loss : 111.15241264396956\n"
     ]
    }
   ],
   "source": [
    "for t in range(iterations) :\n",
    "    running_loss = 0.0\n",
    "    for i , data in enumerate(trainloader, 0) :\n",
    "        # Get inputs and labe l s\n",
    "        inputs , labels = data\n",
    "        Ntr = inputs.shape[0] # Batch size\n",
    "        x_train = inputs.view(Ntr, -1).to(device) # Flatten input to (Ntr, Din)\n",
    "        y_train_onehot = nn.functional.one_hot(labels, K).float().to(device) # Convert labe l s to one=hot # Forward pass\n",
    "        hidden = x_train.mm(w1) + b1\n",
    "        y_pred = hidden.mm(w2) + b2\n",
    "        # Loss calculation (Mean Squared Error with regularization)\n",
    "        loss = (1/Ntr) * torch.sum((y_pred - y_train_onehot) ** 2) + reg * (torch.sum(w1 ** 2) + torch.sum(w2 ** 2))\n",
    "        loss_history.append(loss.item())\n",
    "        running_loss += loss.item()\n",
    "        # Backpropagation\n",
    "        dy_pred = (2.0 / Ntr) * (y_pred - y_train_onehot)\n",
    "        dhidden = dy_pred.mm(w2.t()) \n",
    "        dw2 = hidden.t().mm(dy_pred) + reg * w2\n",
    "        db2 = dy_pred.sum(dim=0)\n",
    "        dw1 = x_train.t().mm(dhidden) + reg * w1\n",
    "        db1 = dhidden.sum(dim=0)\n",
    "        # Parameter update\n",
    "        w2 = w2 - lr * dw2\n",
    "        b2 = b2 - lr * db2\n",
    "        w1 = w1 - lr * dw1\n",
    "        b1 = b1 - lr * db1\n",
    "    print(f\"Epoch {t+1} / {iterations} , Loss : {running_loss/len(trainloader)}\")\n",
    "    # Learning rat e decay\n",
    "    lr *= lr_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del w1, b1, w2, b2, x_train, y_train_onehot, y_pred, loss, dy_pred, dhidden, dw2, db2, dw1, db1\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, Din, H, Dout):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(Din, H)\n",
    "        self.linear2 = nn.Linear(H, Dout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(Din, 100, K).to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 20, Loss: 2.1878856887286546\n",
      "Epoch 2 / 20, Loss: 2.049632305528442\n",
      "Epoch 3 / 20, Loss: 1.97469479146861\n",
      "Epoch 4 / 20, Loss: 1.9242408470091572\n",
      "Epoch 5 / 20, Loss: 1.8867068309777835\n",
      "Epoch 6 / 20, Loss: 1.857231892459452\n",
      "Epoch 7 / 20, Loss: 1.833428738670935\n",
      "Epoch 8 / 20, Loss: 1.813351829007735\n",
      "Epoch 9 / 20, Loss: 1.7960078072563166\n",
      "Epoch 10 / 20, Loss: 1.7810122685331757\n",
      "Epoch 11 / 20, Loss: 1.7676157695852024\n",
      "Epoch 12 / 20, Loss: 1.7555241500118643\n",
      "Epoch 13 / 20, Loss: 1.7445906064331875\n",
      "Epoch 14 / 20, Loss: 1.7345402882179997\n",
      "Epoch 15 / 20, Loss: 1.725008711247435\n",
      "Epoch 16 / 20, Loss: 1.7163191498355537\n",
      "Epoch 17 / 20, Loss: 1.7079995072048135\n",
      "Epoch 18 / 20, Loss: 1.7004681771486445\n",
      "Epoch 19 / 20, Loss: 1.6931195985942946\n",
      "Epoch 20 / 20, Loss: 1.6863281204390816\n"
     ]
    }
   ],
   "source": [
    "for t in range(iterations):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = data\n",
    "        Ntr = inputs.shape[0]  # Batch size\n",
    "        x_train = inputs.view(Ntr, -1).to(device)  # Flatten input to (Ntr, Din)\n",
    "        y_train = labels.to(device)  # Convert labels to one-hot\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x_train)\n",
    "\n",
    "        # Loss calculation\n",
    "        loss_val = loss(y_pred, y_train)\n",
    "        loss_history.append(loss_val.item())\n",
    "        running_loss += loss_val.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {t + 1} / {iterations}, Loss: {running_loss / len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "model.eval()\n",
    "with  torch.inference_mode():\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data\n",
    "        x_test, y_test = inputs.to(device), labels.to(device)\n",
    "        y_pred = model(x_test)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        accuracy += (predicted == y_test).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {accuracy / len(testset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16*5*5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.classifier(self.conv2(self.conv1(x)).view(-1, 16*5*5))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 20, Loss: 1.6081523288158142\n",
      "Epoch 2 / 20, Loss: 1.3175881011167248\n",
      "Epoch 3 / 20, Loss: 1.1851489864628206\n",
      "Epoch 4 / 20, Loss: 1.0968947963156306\n",
      "Epoch 5 / 20, Loss: 1.0310549114235532\n",
      "Epoch 6 / 20, Loss: 0.9732997963539851\n",
      "Epoch 7 / 20, Loss: 0.9319621490616106\n",
      "Epoch 8 / 20, Loss: 0.8904304512593507\n",
      "Epoch 9 / 20, Loss: 0.853049688169915\n",
      "Epoch 10 / 20, Loss: 0.8250834326368833\n",
      "Epoch 11 / 20, Loss: 0.7981342941198453\n",
      "Epoch 12 / 20, Loss: 0.7688879380413758\n",
      "Epoch 13 / 20, Loss: 0.7420874635473856\n",
      "Epoch 14 / 20, Loss: 0.7198617670372824\n",
      "Epoch 15 / 20, Loss: 0.6975821146237415\n",
      "Epoch 16 / 20, Loss: 0.6714369931799894\n",
      "Epoch 17 / 20, Loss: 0.6597335995940619\n",
      "Epoch 18 / 20, Loss: 0.6329375674670428\n",
      "Epoch 19 / 20, Loss: 0.6182287512627155\n",
      "Epoch 20 / 20, Loss: 0.5970457774644774\n"
     ]
    }
   ],
   "source": [
    "for t in range(iterations):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = data\n",
    "        x_train, y_train = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x_train)\n",
    "\n",
    "        # Loss calculation\n",
    "        loss_val = loss(y_pred, y_train)\n",
    "        loss_history.append(loss_val.item())\n",
    "        running_loss += loss_val.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {t + 1} / {iterations}, Loss: {running_loss / len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6313\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "model.eval()\n",
    "with  torch.inference_mode():\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data\n",
    "        x_test, y_test = inputs.to(device), labels.to(device)\n",
    "        y_pred = model(x_test)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        accuracy += (predicted == y_test).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {accuracy / len(testset)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
